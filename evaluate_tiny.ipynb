{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c7df056-d561-497b-9080-599c2d88777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from typing import Any, Dict, List, Union\n",
    "from datasets import load_metric\n",
    "from dataclasses import dataclass\n",
    "from transformers import WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer,  WhisperProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc58209-21ca-46d8-a8ab-7ddc2b8cd349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a26cfc60-e540-4c23-8eb5-b3ac27a06166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d889b211-900f-44a6-8ef6-2ed8bff25062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67b187-9d9f-4076-80e2-1d4eaea84f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_dataset(\"csv\", data_files=\"test.csv\")[\"train\"]\n",
    "gowajee_test = load_dataset(\"csv\", data_files=\"gowajee_test.csv\")[\"train\"]\n",
    "health_test = load_dataset(\"csv\", data_files=\"health_test.csv\")[\"train\"]\n",
    "smart_home_test = load_dataset(\"csv\", data_files=\"smart_home_test.csv\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ddd1634-c8cc-47e0-9057-430d38524802",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"./whisper-tiny-thai/checkpoint-20000\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bce1d3e-828c-4bc8-b23d-abff3e43fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89c11a88-1430-4e02-9221-1b25d52af790",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"Thai\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c6b5f64-e7ad-4997-b27c-5b4110cccd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"Thai\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e0425aa-3021-44b0-9619-3093d1edce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(batch):\n",
    "    audio, sr = librosa.load(\"/mnt/d/data/cv-corpus-13.0-2023-03-09/th/clips/\" + batch[\"path\"], sr=16000)\n",
    "    audio = torch.from_numpy(audio)\n",
    "    batch[\"input_features\"] = audio\n",
    "    batch[\"labels\"] = batch[\"sentence\"]\n",
    "    return batch\n",
    "\n",
    "def load_audio_gowajee(batch):\n",
    "    audio, sr = librosa.load(\"/mnt/d/data/gowajee_v0.9.2/v0.9.2/\" + batch[\"path\"], sr=16000)\n",
    "    audio = torch.from_numpy(audio)\n",
    "    batch[\"input_features\"] = audio\n",
    "    batch[\"labels\"] = batch[\"sentence\"]\n",
    "    return batch\n",
    "\n",
    "def load_audio_smart_home(batch):\n",
    "    audio, sr = librosa.load(\"/mnt/d/data/Dataset/Smarthome/Record/\" + batch[\"path\"], sr=16000)\n",
    "    audio = torch.from_numpy(audio)\n",
    "    batch[\"input_features\"] = audio\n",
    "    batch[\"labels\"] = batch[\"sentence\"]\n",
    "    return batch\n",
    "\n",
    "def load_audio_health(batch):\n",
    "    audio, sr = librosa.load(\"/mnt/d/data/Dataset/Healthcare/Record/\" + batch[\"path\"], sr=16000)\n",
    "    audio = torch.from_numpy(audio)\n",
    "    batch[\"input_features\"] = audio\n",
    "    batch[\"labels\"] = batch[\"sentence\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7be3e14f-9c6b-4a01-bebc-963b85f77f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jui/.cache/huggingface/datasets/csv/default-bf161c8f204ed3d4/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fc77c088ad08ab1d_*_of_00008.arrow\n",
      "Loading cached processed dataset at /home/jui/.cache/huggingface/datasets/csv/default-a21d5816bce7a1de/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-8987b10fb65b3ae4_*_of_00008.arrow\n",
      "Loading cached processed dataset at /home/jui/.cache/huggingface/datasets/csv/default-25577eb20c5736fb/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-819123d436bb3ea4_*_of_00008.arrow\n",
      "Loading cached processed dataset at /home/jui/.cache/huggingface/datasets/csv/default-f5a8e28433f5a511/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-760e3589aa3fd906_*_of_00008.arrow\n"
     ]
    }
   ],
   "source": [
    "test = test.map(load_audio, remove_columns=[\"path\", \"sentence\"], num_proc=8)\n",
    "gowajee_test = gowajee_test.map(load_audio_gowajee, remove_columns=[\"path\", \"sentence\"], num_proc=8)\n",
    "health_test = health_test.map(load_audio_health, remove_columns=[\"path\", \"sentence\"], num_proc=8)\n",
    "smart_home_test = smart_home_test.map(load_audio_smart_home, remove_columns=[\"path\", \"sentence\"], num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7bad19a-b07c-4573-8937-eaf636261428",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        audios = [self.processor.feature_extractor(feature[\"input_features\"], sampling_rate=16000).input_features[0] for feature in features]\n",
    "        sentences = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        input_features = [{\"input_features\": audio} for audio in audios]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        batch[\"labels\"] = sentences\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca962c93-7506-436a-9cd1-be64c1e39f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b47f40fa-b422-4be9-aae1-158c1c16e034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5819/3086256963.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer_metric = load_metric(\"cer\")\n"
     ]
    }
   ],
   "source": [
    "cer_metric = load_metric(\"cer\")\n",
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc8f4df7-af31-48f9-bc82-30b336a253e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/90 [00:00<?, ?it/s]2023-05-27 13:28:04.178649: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-27 13:28:05.270520: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-27 13:28:20.497332: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [06:11<00:00,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wer: 23.144494550920708\n",
      "cer: 6.740680318230452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_dataloader = DataLoader(test, batch_size=32, collate_fn=data_collator)\n",
    "\n",
    "model.eval()\n",
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = (\n",
    "            model.generate(\n",
    "                input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                max_new_tokens=255,\n",
    "                language=\"Thai\"\n",
    "            )\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "        labels = batch[\"labels\"]\n",
    "        transcriptions = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        cer_metric.add_batch(predictions=[pred_str.replace(\" \", \"\") for pred_str in transcriptions], references=[label_str.replace(\" \", \"\") for label_str in labels])\n",
    "\n",
    "        pred_str_newmm = [word_tokenize(text=e, engine='newmm', keep_whitespace=False) for e in transcriptions]\n",
    "        label_str_newmm = [word_tokenize(text=e, engine='newmm', keep_whitespace=False) for e in labels]\n",
    "        wer_metric.add_batch(predictions=pred_str_newmm, references=label_str_newmm)\n",
    "    del generated_tokens, labels, batch\n",
    "    gc.collect()\n",
    "wer = 100 * wer_metric.compute()\n",
    "cer = 100 * cer_metric.compute()\n",
    "print(f\"wer: {wer}\")\n",
    "print(f\"cer: {cer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "392fc988-9b23-4693-8113-71483467dbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:25<00:00,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wer: 24.792643346556076\n",
      "cer: 11.394521138912856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_dataloader = DataLoader(gowajee_test, batch_size=32, collate_fn=data_collator)\n",
    "\n",
    "model.eval()\n",
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = (\n",
    "            model.generate(\n",
    "                input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                max_new_tokens=255,\n",
    "                language=\"Thai\"\n",
    "            )\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "        labels = batch[\"labels\"]\n",
    "        transcriptions = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        cer_metric.add_batch(predictions=[pred_str.replace(\" \", \"\") for pred_str in transcriptions], references=[label_str.replace(\" \", \"\") for label_str in labels])\n",
    "\n",
    "        pred_str_newmm = [word_tokenize(text=e, engine='newmm', keep_whitespace=False) for e in transcriptions]\n",
    "        label_str_newmm = [word_tokenize(text=e, engine='newmm', keep_whitespace=False) for e in labels]\n",
    "        wer_metric.add_batch(predictions=pred_str_newmm, references=label_str_newmm)\n",
    "    del generated_tokens, labels, batch\n",
    "    gc.collect()\n",
    "wer = 100 * wer_metric.compute()\n",
    "cer = 100 * cer_metric.compute()\n",
    "print(f\"wer: {wer}\")\n",
    "print(f\"cer: {cer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2166a1f-00d9-4627-a7c2-43a34b88da28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:36<00:00,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wer: 13.28364752301622\n",
      "cer: 4.143479718404291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_dataloader = DataLoader(health_test, batch_size=32, collate_fn=data_collator)\n",
    "\n",
    "model.eval()\n",
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = (\n",
    "            model.generate(\n",
    "                input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                max_new_tokens=255,\n",
    "                language=\"Thai\"\n",
    "            )\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "        labels = batch[\"labels\"]\n",
    "        transcriptions = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        cer_metric.add_batch(predictions=[pred_str.replace(\" \", \"\") for pred_str in transcriptions], references=[label_str.replace(\" \", \"\") for label_str in labels])\n",
    "\n",
    "        pred_str_newmm = [word_tokenize(text=e, engine='newmm', keep_whitespace=False) for e in transcriptions]\n",
    "        label_str_newmm = [word_tokenize(text=e, engine='newmm', keep_whitespace=False) for e in labels]\n",
    "        wer_metric.add_batch(predictions=pred_str_newmm, references=label_str_newmm)\n",
    "    del generated_tokens, labels, batch\n",
    "    gc.collect()\n",
    "wer = 100 * wer_metric.compute()\n",
    "cer = 100 * cer_metric.compute()\n",
    "print(f\"wer: {wer}\")\n",
    "print(f\"cer: {cer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed2eb1cd-1520-4549-a885-a8422cbd3c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:31<00:00,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wer: 12.992943129929433\n",
      "cer: 3.4138499936545537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_dataloader = DataLoader(smart_home_test, batch_size=32, collate_fn=data_collator)\n",
    "\n",
    "model.eval()\n",
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = (\n",
    "            model.generate(\n",
    "                input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                max_new_tokens=255,\n",
    "                language=\"Thai\"\n",
    "            )\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "        labels = batch[\"labels\"]\n",
    "        transcriptions = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        cer_metric.add_batch(predictions=[pred_str.replace(\" \", \"\") for pred_str in transcriptions], references=[label_str.replace(\" \", \"\") for label_str in labels])\n",
    "\n",
    "        pred_str_newmm = [word_tokenize(text=e, engine='newmm', keep_whitespace=False) for e in transcriptions]\n",
    "        label_str_newmm = [word_tokenize(text=e, engine='newmm', keep_whitespace=False) for e in labels]\n",
    "        wer_metric.add_batch(predictions=pred_str_newmm, references=label_str_newmm)\n",
    "    del generated_tokens, labels, batch\n",
    "    gc.collect()\n",
    "wer = 100 * wer_metric.compute()\n",
    "cer = 100 * cer_metric.compute()\n",
    "print(f\"wer: {wer}\")\n",
    "print(f\"cer: {cer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e23d1-1002-4757-a6b3-0292bece191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(repo_id=\"juierror/whisper-tiny-thai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceeba99-97cf-46a8-86ea-182901a7a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(repo_id=\"juierror/whisper-tiny-thai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef43337-b8fa-4a35-97ea-0cf2e013e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.push_to_hub(repo_id=\"juierror/whisper-tiny-thai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2770419-7941-4620-be04-10f0ed554f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.push_to_hub(repo_id=\"juierror/whisper-tiny-thai\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
