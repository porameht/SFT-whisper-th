{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a7f90-662c-4aa9-8e4b-d9fe8fc25f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from IPython.display import Audio\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf7ab6f-eb20-448f-a35e-cb276342fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "import augment\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7ffa75f-1aa3-4427-9e2b-b6a9fb38a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f6188-603f-46d6-a9db-9e06fa37501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset(\"csv\", data_files=\"train_merge.csv\")[\"train\"]\n",
    "dev = load_dataset(\"csv\", data_files=\"dev_merge.csv\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f16d17f3-57e3-4135-a566-f7795d1dccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3966b9e5-6505-4e55-8b43-c350e632784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"Thai\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "360f8e9f-faba-4af9-ac3d-9c77d6d3c20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 ใครเป็นผู้รับ\n",
      "Decoded w/ special:    <|startoftranscript|><|th|><|transcribe|><|notimestamps|>ใครเป็นผู้รับ<|endoftext|>\n",
      "Decoded w/out special: ใครเป็นผู้รับ\n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "input_str = train[0][\"sentence\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63920a01-efaf-477a-8d83-4c03c5bf1c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"Thai\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b69d2698-21b0-4d8c-90f9-eb5f36877753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_batch(batch):\n",
    "    audios = []\n",
    "    for path in batch[\"path\"]:\n",
    "        audio, sr = librosa.load(path, sr=16000)\n",
    "        audios.append(audio)\n",
    "    batch[\"input_features\"] = audios\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1416b59b-8f5e-43e8-92cc-c4b48d29bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jui/.cache/huggingface/datasets/csv/default-2f1816aee5d0c184/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-1145b0691893d1e3_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/jui/.cache/huggingface/datasets/csv/default-149a15838ff7908b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-e0b7068f2cec181a_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "train = train.map(load_audio_batch, remove_columns=[\"path\", \"sentence\"], batched=True, batch_size=8, num_proc=4)\n",
    "dev = dev.map(load_audio_batch, remove_columns=[\"path\", \"sentence\"], batched=True, batch_size=8, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3145c7f8-ab2d-49ad-8aea-8bf29ac1ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        audios = [feature[\"input_features\"] for feature in features]\n",
    "        sentences = [feature[\"labels\"] for feature in features]\n",
    "        \n",
    "        for i in range(len(audios)):\n",
    "            audio = audios[i]\n",
    "            audio = self.processor.feature_extractor(audio, sampling_rate=16000).input_features[0]\n",
    "            audios[i] = audio\n",
    "\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": audio} for audio in audios]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52f28a1d-61a5-4437-82bc-987c9bf9e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f819edc9-d8c5-4af8-a6d7-962845a81e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d99f8ad3-2817-48aa-9fcb-a57ae4de3cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de2a27b4-014d-4dde-bd17-304ecce79e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-tiny-thai\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=2500,\n",
    "    eval_steps=2500,\n",
    "    logging_steps=2500,\n",
    "    num_train_epochs=5,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "017ad109-f843-46ed-bc74-897be815e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba6ca56-6bee-49f1-9b87-3e90460a89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cdf23c-a1e8-4b92-91fc-b1cf36f9a9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
